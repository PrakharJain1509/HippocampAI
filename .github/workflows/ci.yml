name: CI

on:
  push:
    branches: ["*"]

env:
  PYTHONUNBUFFERED: "1"
  FORCE_COLOR: "1"

jobs:
  test:
    name: Test and Lint (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]

    services:
      qdrant:
        # (optional but recommended) pin image to a version or digest instead of :latest
        image: qdrant/qdrant@sha256:f8f833b5c8d215bb94ff20b7c68cb5f0ca2f4516c870e321a1b655141665c105
        ports:
          - 6333:6333
          - 6334:6334
        options: >-
          --health-cmd="curl -fsS http://localhost:6333/readyz || exit 1"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=60

      ollama:
        # Pin to a specific tag; update as needed when you bump locally
        image: ghcr.io/ollama/ollama:0.4.6
        ports:
          - 11434:11434
        options: >-
          --health-cmd="curl -fsS http://localhost:11434/api/tags || exit 1"
          --health-interval=5s
          --health-timeout=3s
          --health-retries=60

    steps:
      - name: Checkout repository
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955   # v4.3.0

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache Python dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830       # v4.3.0
        with:
          path: |
            ~/.cache/pip
            ${{ env.pythonLocation }}
          key: ${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml', '**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-python-${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e '.[dev,api]'

      - name: Wait for Qdrant to be ready
        run: |
          echo "Waiting for Qdrant to be ready..."
          for i in {1..60}; do
            if curl -fsS http://localhost:6333/readyz > /dev/null; then
              echo "Qdrant is ready!"
              curl -s http://localhost:6333/ | head -20
              exit 0
            fi
            echo "Attempt $i/60: Qdrant not ready yet, waiting..."
            sleep 2
          done
          echo "Qdrant failed to become ready"
          exit 1

      - name: Wait for Ollama to be ready
        run: |
          echo "Waiting for Ollama to be ready..."
          for i in {1..60}; do
            if curl -fsS http://localhost:11434/api/tags > /dev/null; then
              echo "Ollama is ready!"
              break
            fi
            echo "Attempt $i/60: Ollama not ready yet, waiting..."
            sleep 2
          done

      - name: Pull embedding model into Ollama
        run: |
          set -e
          MODEL="nomic-embed-text"
          echo "Pulling ${MODEL}..."
          # Fire the pull; Ollama streams progress
          curl -fsS -X POST http://localhost:11434/api/pull -d "{\"name\":\"${MODEL}\"}"
          echo "Verifying model presence..."
          for i in {1..60}; do
            if curl -fsS http://localhost:11434/api/tags | grep -q "\"name\":\"${MODEL}\""; then
              echo "Model ${MODEL} ready."
              exit 0
            fi
            echo "Attempt $i/60: model not visible yet, waiting..."
            sleep 2
          done
          echo "Model ${MODEL} failed to appear in /api/tags"
          curl -s http://localhost:11434/api/tags || true
          exit 1

      - name: Lint with Ruff
        run: |
          ruff check . --output-format=github
          ruff format . --check

      - name: Run Tests
        run: |
          pytest -v --maxfail=1 --disable-warnings --cov=src --cov-report=term-missing
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
          QDRANT_URL: http://localhost:6333
          # Make HippocampAI use local Ollama for embeddings/generation
          HIPPOCAMP_PROVIDER: ollama
          HIPPOCAMP_OLLAMA_BASE_URL: http://localhost:11434
          HIPPOCAMP_EMBED_MODEL: nomic-embed-text
          # Optional: if your code reads OLLAMA_HOST
          OLLAMA_HOST: http://localhost:11434

  build:
    name: Build Package
    runs-on: ubuntu-latest
    needs: [test]

    steps:
      - name: Checkout repository
        uses: actions/checkout@08eba0b27e820071cde6df949e0beb9ba4906955   # v4.3.0

      - name: Set up Python
        uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065 # v5.6.0
        with:
          python-version: "3.11"

      - name: Cache Python dependencies
        uses: actions/cache@0057852bfaa89a56745cba8c7296529d2fc39830       # v4.3.0
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-build-${{ hashFiles('**/pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-build-

      - name: Install build tools
        run: |
          python -m pip install --upgrade pip
          pip install build twine

      - name: Build package
        run: python -m build

      - name: Validate build with Twine
        run: twine check dist/*
